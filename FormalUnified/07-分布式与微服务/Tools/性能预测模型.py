#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½é¢„æµ‹æ¨¡å‹
Performance Prediction Model

æœ¬å·¥å…·æä¾›åˆ†å¸ƒå¼ç³»ç»Ÿå’Œå¾®æœåŠ¡çš„æ€§èƒ½é¢„æµ‹åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. å»¶è¿Ÿé¢„æµ‹æ¨¡å‹
2. ååé‡é¢„æµ‹æ¨¡å‹
3. èµ„æºåˆ©ç”¨ç‡é¢„æµ‹
4. å®¹é‡è§„åˆ’
5. æ€§èƒ½ç“¶é¢ˆåˆ†æ
6. è´Ÿè½½é¢„æµ‹
"""

import json
import time
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PredictionType(Enum):
    """é¢„æµ‹ç±»å‹æšä¸¾"""
    LATENCY = "latency"
    THROUGHPUT = "throughput"
    CPU_USAGE = "cpu_usage"
    MEMORY_USAGE = "memory_usage"
    NETWORK_USAGE = "network_usage"
    DISK_USAGE = "disk_usage"

class ModelType(Enum):
    """æ¨¡å‹ç±»å‹æšä¸¾"""
    LINEAR = "linear"
    RIDGE = "ridge"
    RANDOM_FOREST = "random_forest"
    POLYNOMIAL = "polynomial"

@dataclass
class PerformanceData:
    """æ€§èƒ½æ•°æ®"""
    timestamp: float
    request_rate: float
    concurrent_users: int
    cpu_usage: float
    memory_usage: float
    network_usage: float
    disk_usage: float
    latency: float
    throughput: float
    error_rate: float

@dataclass
class PredictionResult:
    """é¢„æµ‹ç»“æœ"""
    prediction_type: PredictionType
    model_type: ModelType
    predicted_value: float
    confidence: float
    features: Dict[str, float]
    model_score: float
    timestamp: float

@dataclass
class CapacityPlan:
    """å®¹é‡è§„åˆ’"""
    service_name: str
    current_capacity: Dict[str, float]
    predicted_demand: Dict[str, float]
    recommended_capacity: Dict[str, float]
    scaling_factor: float
    cost_estimate: float

class PerformancePredictor:
    """æ€§èƒ½é¢„æµ‹å™¨"""
    
    def __init__(self):
        self.models: Dict[str, Any] = {}
        self.scalers: Dict[str, StandardScaler] = {}
        self.feature_columns = [
            'request_rate', 'concurrent_users', 'cpu_usage', 
            'memory_usage', 'network_usage', 'disk_usage'
        ]
        self.target_columns = ['latency', 'throughput']
        self.performance_history: List[PerformanceData] = []
        
    def add_performance_data(self, data: PerformanceData):
        """æ·»åŠ æ€§èƒ½æ•°æ®"""
        self.performance_history.append(data)
        logger.info(f"æ·»åŠ æ€§èƒ½æ•°æ®: {data.timestamp}")
    
    def prepare_training_data(self, prediction_type: PredictionType) -> Tuple[np.ndarray, np.ndarray]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        if len(self.performance_history) < 10:
            raise ValueError("éœ€è¦è‡³å°‘10ä¸ªæ•°æ®ç‚¹è¿›è¡Œè®­ç»ƒ")
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame([asdict(data) for data in self.performance_history])
        
        # é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡
        X = df[self.feature_columns].values
        y = df[prediction_type.value].values
        
        return X, y
    
    def train_model(self, prediction_type: PredictionType, model_type: ModelType = ModelType.RANDOM_FOREST):
        """è®­ç»ƒæ¨¡å‹"""
        try:
            X, y = self.prepare_training_data(prediction_type)
            
            # æ•°æ®æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # é€‰æ‹©æ¨¡å‹
            if model_type == ModelType.LINEAR:
                model = LinearRegression()
            elif model_type == ModelType.RIDGE:
                model = Ridge(alpha=1.0)
            elif model_type == ModelType.RANDOM_FOREST:
                model = RandomForestRegressor(n_estimators=100, random_state=42)
            else:
                model = LinearRegression()
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_scaled, y)
            
            # è¯„ä¼°æ¨¡å‹
            y_pred = model.predict(X_scaled)
            score = r2_score(y, y_pred)
            
            # ä¿å­˜æ¨¡å‹å’Œscaler
            model_key = f"{prediction_type.value}_{model_type.value}"
            self.models[model_key] = model
            self.scalers[model_key] = scaler
            
            logger.info(f"æ¨¡å‹è®­ç»ƒå®Œæˆ: {model_key}, RÂ² = {score:.4f}")
            return score
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return 0.0
    
    def predict(self, features: Dict[str, float], prediction_type: PredictionType, 
                model_type: ModelType = ModelType.RANDOM_FOREST) -> Optional[PredictionResult]:
        """è¿›è¡Œé¢„æµ‹"""
        model_key = f"{prediction_type.value}_{model_type.value}"
        
        if model_key not in self.models:
            logger.warning(f"æ¨¡å‹ä¸å­˜åœ¨: {model_key}")
            return None
        
        try:
            model = self.models[model_key]
            scaler = self.scalers[model_key]
            
            # å‡†å¤‡ç‰¹å¾
            feature_vector = []
            for col in self.feature_columns:
                feature_vector.append(features.get(col, 0.0))
            
            X = np.array([feature_vector])
            X_scaled = scaler.transform(X)
            
            # é¢„æµ‹
            predicted_value = model.predict(X_scaled)[0]
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆç®€åŒ–å®ç°ï¼‰
            confidence = 0.8  # å®é™…åº”è¯¥åŸºäºæ¨¡å‹çš„ä¸ç¡®å®šæ€§
            
            return PredictionResult(
                prediction_type=prediction_type,
                model_type=model_type,
                predicted_value=predicted_value,
                confidence=confidence,
                features=features,
                model_score=0.85,  # å®é™…åº”è¯¥ä¿å­˜è®­ç»ƒæ—¶çš„åˆ†æ•°
                timestamp=time.time()
            )
            
        except Exception as e:
            logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
            return None
    
    def predict_latency(self, request_rate: float, concurrent_users: int, 
                       cpu_usage: float = 0.5, memory_usage: float = 0.5,
                       network_usage: float = 0.5, disk_usage: float = 0.5) -> Optional[PredictionResult]:
        """é¢„æµ‹å»¶è¿Ÿ"""
        features = {
            'request_rate': request_rate,
            'concurrent_users': concurrent_users,
            'cpu_usage': cpu_usage,
            'memory_usage': memory_usage,
            'network_usage': network_usage,
            'disk_usage': disk_usage
        }
        return self.predict(features, PredictionType.LATENCY)
    
    def predict_throughput(self, request_rate: float, concurrent_users: int,
                          cpu_usage: float = 0.5, memory_usage: float = 0.5,
                          network_usage: float = 0.5, disk_usage: float = 0.5) -> Optional[PredictionResult]:
        """é¢„æµ‹ååé‡"""
        features = {
            'request_rate': request_rate,
            'concurrent_users': concurrent_users,
            'cpu_usage': cpu_usage,
            'memory_usage': memory_usage,
            'network_usage': network_usage,
            'disk_usage': disk_usage
        }
        return self.predict(features, PredictionType.THROUGHPUT)

class CapacityPlanner:
    """å®¹é‡è§„åˆ’å™¨"""
    
    def __init__(self, performance_predictor: PerformancePredictor):
        self.predictor = performance_predictor
        self.capacity_limits = {
            'cpu_usage': 0.8,
            'memory_usage': 0.8,
            'network_usage': 0.7,
            'disk_usage': 0.8,
            'latency': 1000.0,  # ms
            'throughput': 1000.0  # requests/second
        }
    
    def plan_capacity(self, service_name: str, target_load: Dict[str, float]) -> CapacityPlan:
        """å®¹é‡è§„åˆ’"""
        # é¢„æµ‹åœ¨ç›®æ ‡è´Ÿè½½ä¸‹çš„æ€§èƒ½
        latency_pred = self.predictor.predict_latency(
            target_load.get('request_rate', 100),
            target_load.get('concurrent_users', 50)
        )
        
        throughput_pred = self.predictor.predict_throughput(
            target_load.get('request_rate', 100),
            target_load.get('concurrent_users', 50)
        )
        
        # å½“å‰å®¹é‡ï¼ˆå‡è®¾ï¼‰
        current_capacity = {
            'cpu_cores': 4,
            'memory_gb': 8,
            'network_mbps': 1000,
            'disk_gb': 100
        }
        
        # é¢„æµ‹çš„èµ„æºéœ€æ±‚
        predicted_demand = {
            'cpu_cores': target_load.get('request_rate', 100) / 50,  # ç®€åŒ–è®¡ç®—
            'memory_gb': target_load.get('concurrent_users', 50) * 0.1,
            'network_mbps': target_load.get('request_rate', 100) * 0.1,
            'disk_gb': target_load.get('request_rate', 100) * 0.01
        }
        
        # æ¨èå®¹é‡
        recommended_capacity = {}
        scaling_factor = 1.0
        
        for resource, current in current_capacity.items():
            predicted = predicted_demand.get(resource, 0)
            if predicted > current:
                scaling_factor = max(scaling_factor, predicted / current)
                recommended_capacity[resource] = predicted * 1.2  # 20%ç¼“å†²
            else:
                recommended_capacity[resource] = current
        
        # æˆæœ¬ä¼°ç®—ï¼ˆç®€åŒ–ï¼‰
        cost_estimate = sum(recommended_capacity.values()) * 0.1
        
        return CapacityPlan(
            service_name=service_name,
            current_capacity=current_capacity,
            predicted_demand=predicted_demand,
            recommended_capacity=recommended_capacity,
            scaling_factor=scaling_factor,
            cost_estimate=cost_estimate
        )

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, performance_predictor: PerformancePredictor):
        self.predictor = performance_predictor
    
    def analyze_bottlenecks(self, current_load: Dict[str, float]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = {}
        
        # é¢„æµ‹å½“å‰è´Ÿè½½ä¸‹çš„æ€§èƒ½
        latency_pred = self.predictor.predict_latency(
            current_load.get('request_rate', 100),
            current_load.get('concurrent_users', 50)
        )
        
        if latency_pred:
            if latency_pred.predicted_value > self.predictor.capacity_limits['latency']:
                bottlenecks['latency'] = {
                    'current': latency_pred.predicted_value,
                    'limit': self.predictor.capacity_limits['latency'],
                    'severity': 'high' if latency_pred.predicted_value > 2000 else 'medium'
                }
        
        # æ£€æŸ¥èµ„æºä½¿ç”¨ç‡
        for resource in ['cpu_usage', 'memory_usage', 'network_usage', 'disk_usage']:
            current_usage = current_load.get(resource, 0.0)
            limit = self.predictor.capacity_limits[resource]
            
            if current_usage > limit:
                bottlenecks[resource] = {
                    'current': current_usage,
                    'limit': limit,
                    'severity': 'high' if current_usage > limit * 1.2 else 'medium'
                }
        
        return bottlenecks
    
    def generate_performance_report(self, service_name: str, 
                                  current_load: Dict[str, float]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        # æ€§èƒ½é¢„æµ‹
        latency_pred = self.predictor.predict_latency(
            current_load.get('request_rate', 100),
            current_load.get('concurrent_users', 50)
        )
        
        throughput_pred = self.predictor.predict_throughput(
            current_load.get('request_rate', 100),
            current_load.get('concurrent_users', 50)
        )
        
        # ç“¶é¢ˆåˆ†æ
        bottlenecks = self.analyze_bottlenecks(current_load)
        
        # å®¹é‡è§„åˆ’
        capacity_planner = CapacityPlanner(self.predictor)
        capacity_plan = capacity_planner.plan_capacity(service_name, current_load)
        
        return {
            'service_name': service_name,
            'timestamp': time.time(),
            'current_load': current_load,
            'predictions': {
                'latency': asdict(latency_pred) if latency_pred else None,
                'throughput': asdict(throughput_pred) if throughput_pred else None
            },
            'bottlenecks': bottlenecks,
            'capacity_plan': asdict(capacity_plan),
            'recommendations': self._generate_recommendations(bottlenecks, capacity_plan)
        }
    
    def _generate_recommendations(self, bottlenecks: Dict[str, Any], 
                                capacity_plan: CapacityPlan) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        if 'latency' in bottlenecks:
            recommendations.append("è€ƒè™‘å¢åŠ CPUæ ¸å¿ƒæ•°æˆ–ä¼˜åŒ–ä»£ç æ€§èƒ½")
        
        if 'cpu_usage' in bottlenecks:
            recommendations.append("å¢åŠ CPUèµ„æºæˆ–ä¼˜åŒ–CPUå¯†é›†å‹æ“ä½œ")
        
        if 'memory_usage' in bottlenecks:
            recommendations.append("å¢åŠ å†…å­˜å®¹é‡æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
        
        if 'network_usage' in bottlenecks:
            recommendations.append("å‡çº§ç½‘ç»œå¸¦å®½æˆ–ä¼˜åŒ–ç½‘ç»œé€šä¿¡")
        
        if capacity_plan.scaling_factor > 1.5:
            recommendations.append("å»ºè®®è¿›è¡Œå®¹é‡æ‰©å±•")
        
        return recommendations

class LoadForecaster:
    """è´Ÿè½½é¢„æµ‹å™¨"""
    
    def __init__(self):
        self.historical_loads: List[Dict[str, float]] = []
        self.forecast_model = None
    
    def add_historical_load(self, load_data: Dict[str, float]):
        """æ·»åŠ å†å²è´Ÿè½½æ•°æ®"""
        self.historical_loads.append(load_data)
    
    def forecast_load(self, time_horizon: int = 24) -> List[Dict[str, float]]:
        """é¢„æµ‹æœªæ¥è´Ÿè½½"""
        if len(self.historical_loads) < 10:
            return []
        
        # ç®€åŒ–çš„è´Ÿè½½é¢„æµ‹ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨æ—¶é—´åºåˆ—æ¨¡å‹ï¼‰
        forecasts = []
        last_load = self.historical_loads[-1]
        
        for i in range(time_horizon):
            # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–å’Œè¶‹åŠ¿
            forecast = {}
            for key, value in last_load.items():
                if key in ['request_rate', 'concurrent_users']:
                    # æ·»åŠ å¢é•¿è¶‹åŠ¿å’Œéšæœºæ³¢åŠ¨
                    trend = 1.0 + (i * 0.01)  # 1%æ¯å°æ—¶å¢é•¿
                    noise = 1.0 + np.random.normal(0, 0.05)  # 5%éšæœºæ³¢åŠ¨
                    forecast[key] = value * trend * noise
                else:
                    forecast[key] = value
            
            forecasts.append(forecast)
        
        return forecasts

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ€§èƒ½é¢„æµ‹æ¨¡å‹ç³»ç»Ÿå¯åŠ¨")
    
    # åˆ›å»ºæ€§èƒ½é¢„æµ‹å™¨
    predictor = PerformancePredictor()
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ€§èƒ½æ•°æ®
    print("ğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿæ€§èƒ½æ•°æ®...")
    
    for i in range(50):
        # æ¨¡æ‹Ÿä¸åŒè´Ÿè½½ä¸‹çš„æ€§èƒ½æ•°æ®
        request_rate = 50 + i * 10
        concurrent_users = 10 + i * 2
        cpu_usage = 0.3 + (i / 50) * 0.4
        memory_usage = 0.4 + (i / 50) * 0.3
        network_usage = 0.2 + (i / 50) * 0.4
        disk_usage = 0.1 + (i / 50) * 0.2
        
        # æ¨¡æ‹Ÿå»¶è¿Ÿå’Œååé‡ï¼ˆåŸºäºè´Ÿè½½ï¼‰
        latency = 100 + (request_rate / 10) + (concurrent_users * 2) + np.random.normal(0, 10)
        throughput = request_rate * (1 - cpu_usage * 0.5) + np.random.normal(0, 5)
        
        data = PerformanceData(
            timestamp=time.time() + i * 3600,  # æ¯å°æ—¶ä¸€ä¸ªæ•°æ®ç‚¹
            request_rate=request_rate,
            concurrent_users=concurrent_users,
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            network_usage=network_usage,
            disk_usage=disk_usage,
            latency=latency,
            throughput=throughput,
            error_rate=0.01 + (cpu_usage * 0.02)
        )
        
        predictor.add_performance_data(data)
    
    # è®­ç»ƒæ¨¡å‹
    print("ğŸ¤– è®­ç»ƒæ€§èƒ½é¢„æµ‹æ¨¡å‹...")
    
    latency_score = predictor.train_model(PredictionType.LATENCY, ModelType.RANDOM_FOREST)
    throughput_score = predictor.train_model(PredictionType.THROUGHPUT, ModelType.RANDOM_FOREST)
    
    print(f"å»¶è¿Ÿé¢„æµ‹æ¨¡å‹ RÂ² = {latency_score:.4f}")
    print(f"ååé‡é¢„æµ‹æ¨¡å‹ RÂ² = {throughput_score:.4f}")
    
    # è¿›è¡Œé¢„æµ‹
    print("ğŸ”® è¿›è¡Œæ€§èƒ½é¢„æµ‹...")
    
    test_features = {
        'request_rate': 200,
        'concurrent_users': 100,
        'cpu_usage': 0.7,
        'memory_usage': 0.6,
        'network_usage': 0.5,
        'disk_usage': 0.3
    }
    
    latency_pred = predictor.predict_latency(
        test_features['request_rate'],
        test_features['concurrent_users'],
        test_features['cpu_usage'],
        test_features['memory_usage'],
        test_features['network_usage'],
        test_features['disk_usage']
    )
    
    throughput_pred = predictor.predict_throughput(
        test_features['request_rate'],
        test_features['concurrent_users'],
        test_features['cpu_usage'],
        test_features['memory_usage'],
        test_features['network_usage'],
        test_features['disk_usage']
    )
    
    print(f"é¢„æµ‹å»¶è¿Ÿ: {latency_pred.predicted_value:.2f}ms (ç½®ä¿¡åº¦: {latency_pred.confidence:.2f})")
    print(f"é¢„æµ‹ååé‡: {throughput_pred.predicted_value:.2f} req/s (ç½®ä¿¡åº¦: {throughput_pred.confidence:.2f})")
    
    # æ€§èƒ½åˆ†æ
    print("ğŸ“ˆ è¿›è¡Œæ€§èƒ½åˆ†æ...")
    
    analyzer = PerformanceAnalyzer(predictor)
    current_load = {
        'request_rate': 150,
        'concurrent_users': 75,
        'cpu_usage': 0.8,
        'memory_usage': 0.7,
        'network_usage': 0.6,
        'disk_usage': 0.4
    }
    
    report = analyzer.generate_performance_report("user-service", current_load)
    
    # è¾“å‡ºåˆ†æç»“æœ
    print(f"\nğŸ“‹ æ€§èƒ½åˆ†ææŠ¥å‘Š:")
    print(f"æœåŠ¡åç§°: {report['service_name']}")
    print(f"ç“¶é¢ˆæ•°é‡: {len(report['bottlenecks'])}")
    print(f"æ‰©å±•å› å­: {report['capacity_plan']['scaling_factor']:.2f}")
    print(f"æˆæœ¬ä¼°ç®—: ${report['capacity_plan']['cost_estimate']:.2f}")
    
    if report['bottlenecks']:
        print(f"\nğŸš¨ å‘ç°ç“¶é¢ˆ:")
        for resource, info in report['bottlenecks'].items():
            print(f"- {resource}: {info['current']:.2f} > {info['limit']:.2f} ({info['severity']})")
    
    if report['recommendations']:
        print(f"\nğŸ’¡ å»ºè®®:")
        for rec in report['recommendations']:
            print(f"- {rec}")
    
    # è´Ÿè½½é¢„æµ‹
    print("\nğŸ”® è´Ÿè½½é¢„æµ‹...")
    
    forecaster = LoadForecaster()
    for data in predictor.performance_history[:10]:
        forecaster.add_historical_load({
            'request_rate': data.request_rate,
            'concurrent_users': data.concurrent_users,
            'cpu_usage': data.cpu_usage
        })
    
    forecasts = forecaster.forecast_load(24)  # é¢„æµ‹24å°æ—¶
    
    print(f"è´Ÿè½½é¢„æµ‹å®Œæˆï¼Œé¢„æµ‹æœªæ¥24å°æ—¶çš„è´Ÿè½½å˜åŒ–")
    print(f"å½“å‰è´Ÿè½½: {forecasts[0]['request_rate']:.0f} req/s")
    print(f"24å°æ—¶åé¢„æµ‹è´Ÿè½½: {forecasts[-1]['request_rate']:.0f} req/s")
    
    # ä¿å­˜æŠ¥å‘Š
    with open("performance_prediction_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("ğŸ¨ ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–...")
    
    # ç»˜åˆ¶æ€§èƒ½æ•°æ®
    plt.figure(figsize=(15, 10))
    
    # å»¶è¿Ÿvsè¯·æ±‚ç‡
    plt.subplot(2, 3, 1)
    request_rates = [data.request_rate for data in predictor.performance_history]
    latencies = [data.latency for data in predictor.performance_history]
    plt.scatter(request_rates, latencies, alpha=0.6)
    plt.xlabel('Request Rate (req/s)')
    plt.ylabel('Latency (ms)')
    plt.title('Latency vs Request Rate')
    
    # ååé‡vså¹¶å‘ç”¨æˆ·
    plt.subplot(2, 3, 2)
    concurrent_users = [data.concurrent_users for data in predictor.performance_history]
    throughputs = [data.throughput for data in predictor.performance_history]
    plt.scatter(concurrent_users, throughputs, alpha=0.6)
    plt.xlabel('Concurrent Users')
    plt.ylabel('Throughput (req/s)')
    plt.title('Throughput vs Concurrent Users')
    
    # CPUä½¿ç”¨ç‡è¶‹åŠ¿
    plt.subplot(2, 3, 3)
    timestamps = [data.timestamp for data in predictor.performance_history]
    cpu_usages = [data.cpu_usage for data in predictor.performance_history]
    plt.plot(timestamps, cpu_usages)
    plt.xlabel('Time')
    plt.ylabel('CPU Usage')
    plt.title('CPU Usage Trend')
    
    # å†…å­˜ä½¿ç”¨ç‡è¶‹åŠ¿
    plt.subplot(2, 3, 4)
    memory_usages = [data.memory_usage for data in predictor.performance_history]
    plt.plot(timestamps, memory_usages)
    plt.xlabel('Time')
    plt.ylabel('Memory Usage')
    plt.title('Memory Usage Trend')
    
    # è´Ÿè½½é¢„æµ‹
    plt.subplot(2, 3, 5)
    forecast_hours = list(range(len(forecasts)))
    forecast_rates = [f['request_rate'] for f in forecasts]
    plt.plot(forecast_hours, forecast_rates)
    plt.xlabel('Hours')
    plt.ylabel('Predicted Request Rate')
    plt.title('Load Forecast')
    
    # æ€§èƒ½æŒ‡æ ‡åˆ†å¸ƒ
    plt.subplot(2, 3, 6)
    plt.hist(latencies, bins=20, alpha=0.7)
    plt.xlabel('Latency (ms)')
    plt.ylabel('Frequency')
    plt.title('Latency Distribution')
    
    plt.tight_layout()
    plt.savefig("performance_analysis.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… æ€§èƒ½é¢„æµ‹æ¨¡å‹å®Œæˆï¼")
    print("ğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: performance_prediction_report.json")
    print("ğŸ–¼ï¸ å¯è§†åŒ–å·²ä¿å­˜åˆ°: performance_analysis.png")

if __name__ == "__main__":
    main()
