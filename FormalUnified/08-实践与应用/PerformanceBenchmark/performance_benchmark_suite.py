#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶
Performance Benchmark Suite

ä¸ºFormalUnifiedå·¥å…·é“¾æä¾›å…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ç†è®ºåŠ è½½ã€ä»£ç ç”Ÿæˆã€éªŒè¯å¼•æ“ç­‰æ ¸å¿ƒåŠŸèƒ½çš„æ€§èƒ½è¯„ä¼°
"""

import time
import psutil
import threading
import multiprocessing
import json
import yaml
import logging
import statistics
from typing import Dict, List, Any, Optional, Tuple, Callable
from pathlib import Path
from datetime import datetime
import sys
import os
import gc
import asyncio
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

# å¯¼å…¥å·¥å…·
try:
    from UnifiedModelingTool.unified_modeling_tool import UnifiedModelingTool
    from AutomatedCodeGenerator.automated_code_generator import AutomatedCodeGenerator
    from CrossTheoryVerificationEngine import CrossTheoryVerificationEngine
    from TheoryToPractice.mapping_tool import EnhancedTheoryToPracticeMapper
    from TestingFramework.comprehensive_test_suite import ComprehensiveTestSuite
except ImportError as e:
    print(f"è­¦å‘Šï¼šéƒ¨åˆ†å·¥å…·å¯¼å…¥å¤±è´¥: {e}")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class BenchmarkConfig:
    """åŸºå‡†æµ‹è¯•é…ç½®"""
    # æµ‹è¯•åç§°
    name: str
    # æµ‹è¯•æè¿°
    description: str
    # æµ‹è¯•å‡½æ•°
    test_function: Callable
    # æµ‹è¯•å‚æ•°
    parameters: Dict[str, Any] = field(default_factory=dict)
    # é‡å¤æ¬¡æ•°
    iterations: int = 10
    # é¢„çƒ­æ¬¡æ•°
    warmup_iterations: int = 3
    # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    timeout: float = 300.0
    # æ˜¯å¦å¯ç”¨å†…å­˜ç›‘æ§
    enable_memory_monitoring: bool = True
    # æ˜¯å¦å¯ç”¨CPUç›‘æ§
    enable_cpu_monitoring: bool = True
    # æ˜¯å¦å¯ç”¨å¤šçº¿ç¨‹æµ‹è¯•
    enable_multithreading: bool = False
    # çº¿ç¨‹æ•°
    thread_count: int = 4
    # æ˜¯å¦å¯ç”¨å¤šè¿›ç¨‹æµ‹è¯•
    enable_multiprocessing: bool = False
    # è¿›ç¨‹æ•°
    process_count: int = 4

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    # æµ‹è¯•åç§°
    name: str
    # æµ‹è¯•é…ç½®
    config: BenchmarkConfig
    # æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
    execution_times: List[float]
    # å¹³å‡æ‰§è¡Œæ—¶é—´
    avg_execution_time: float
    # æœ€å°æ‰§è¡Œæ—¶é—´
    min_execution_time: float
    # æœ€å¤§æ‰§è¡Œæ—¶é—´
    max_execution_time: float
    # æ ‡å‡†å·®
    std_deviation: float
    # å†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰
    memory_usage: Optional[float] = None
    # å³°å€¼å†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰
    peak_memory_usage: Optional[float] = None
    # CPUä½¿ç”¨ç‡ï¼ˆ%ï¼‰
    cpu_usage: Optional[float] = None
    # å³°å€¼CPUä½¿ç”¨ç‡ï¼ˆ%ï¼‰
    peak_cpu_usage: Optional[float] = None
    # æ˜¯å¦æˆåŠŸ
    success: bool = True
    # é”™è¯¯ä¿¡æ¯
    error_message: Optional[str] = None
    # æµ‹è¯•æ—¶é—´æˆ³
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    # ååé‡ï¼ˆæ“ä½œ/ç§’ï¼‰
    throughput: float
    # å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    latency_ms: float
    # å†…å­˜æ•ˆç‡ï¼ˆMB/æ“ä½œï¼‰
    memory_efficiency: float
    # CPUæ•ˆç‡ï¼ˆ%ï¼‰
    cpu_efficiency: float
    # å¯æ‰©å±•æ€§è¯„åˆ†
    scalability_score: float
    # ç¨³å®šæ€§è¯„åˆ†
    stability_score: float

class PerformanceBenchmarkSuite:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.results: List[BenchmarkResult] = []
        self.configs: List[BenchmarkConfig] = []
        self.tools = {}
        self._initialize_tools()
        self._setup_benchmarks()
        
    def _initialize_tools(self):
        """åˆå§‹åŒ–å·¥å…·"""
        try:
            self.tools['modeling'] = UnifiedModelingTool()
            logger.info("âœ… ç»Ÿä¸€å»ºæ¨¡å·¥å…·åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âŒ ç»Ÿä¸€å»ºæ¨¡å·¥å…·åˆå§‹åŒ–å¤±è´¥: {e}")
        
        try:
            self.tools['code_generator'] = AutomatedCodeGenerator()
            logger.info("âœ… è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âŒ è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        try:
            self.tools['verifier'] = CrossTheoryVerificationEngine()
            logger.info("âœ… è·¨ç†è®ºéªŒè¯å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âŒ è·¨ç†è®ºéªŒè¯å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
        
        try:
            self.tools['mapper'] = EnhancedTheoryToPracticeMapper()
            logger.info("âœ… ç†è®ºåˆ°å®è·µæ˜ å°„å·¥å…·åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âŒ ç†è®ºåˆ°å®è·µæ˜ å°„å·¥å…·åˆå§‹åŒ–å¤±è´¥: {e}")
        
        try:
            self.tools['tester'] = ComprehensiveTestSuite()
            logger.info("âœ… ç»¼åˆæµ‹è¯•å¥—ä»¶åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âŒ ç»¼åˆæµ‹è¯•å¥—ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _setup_benchmarks(self):
        """è®¾ç½®åŸºå‡†æµ‹è¯•é…ç½®"""
        # ç†è®ºåŠ è½½æ€§èƒ½æµ‹è¯•
        self.configs.append(BenchmarkConfig(
            name="ç†è®ºåŠ è½½æ€§èƒ½æµ‹è¯•",
            description="æµ‹è¯•ç†è®ºæ–‡æ¡£åŠ è½½å’Œè§£æçš„æ€§èƒ½",
            test_function=self._benchmark_theory_loading,
            parameters={
                "theory_files": ["01-å“²å­¦åŸºç¡€ç†è®º", "02-æ•°å­¦ç†è®ºä½“ç³»", "03-å½¢å¼è¯­è¨€ç†è®ºä½“ç³»"],
                "file_count": 10
            },
            iterations=20,
            warmup_iterations=5
        ))
        
        # ä»£ç ç”Ÿæˆæ€§èƒ½æµ‹è¯•
        self.configs.append(BenchmarkConfig(
            name="ä»£ç ç”Ÿæˆæ€§èƒ½æµ‹è¯•",
            description="æµ‹è¯•å¤šè¯­è¨€ä»£ç ç”Ÿæˆçš„æ€§èƒ½",
            test_function=self._benchmark_code_generation,
            parameters={
                "languages": ["python", "rust", "go", "typescript"],
                "patterns": ["mvc", "repository", "factory"],
                "complexity": "medium"
            },
            iterations=15,
            warmup_iterations=3
        ))
        
        # æ¨¡å‹éªŒè¯æ€§èƒ½æµ‹è¯•
        self.configs.append(BenchmarkConfig(
            name="æ¨¡å‹éªŒè¯æ€§èƒ½æµ‹è¯•",
            description="æµ‹è¯•æ¨¡å‹éªŒè¯å’Œä¸€è‡´æ€§æ£€æŸ¥çš„æ€§èƒ½",
            test_function=self._benchmark_model_verification,
            parameters={
                "model_types": ["uml", "bpmn", "petri_net", "state_machine"],
                "model_size": "large"
            },
            iterations=12,
            warmup_iterations=3
        ))
        
        # è·¨ç†è®ºéªŒè¯æ€§èƒ½æµ‹è¯•
        self.configs.append(BenchmarkConfig(
            name="è·¨ç†è®ºéªŒè¯æ€§èƒ½æµ‹è¯•",
            description="æµ‹è¯•è·¨ç†è®ºä½“ç³»éªŒè¯çš„æ€§èƒ½",
            test_function=self._benchmark_cross_theory_verification,
            parameters={
                "theory_pairs": [
                    ("å“²å­¦åŸºç¡€ç†è®º", "æ•°å­¦ç†è®ºä½“ç³»"),
                    ("å½¢å¼è¯­è¨€ç†è®ºä½“ç³»", "ç¼–ç¨‹è¯­è¨€ç†è®ºä½“ç³»"),
                    ("å½¢å¼æ¨¡å‹ç†è®ºä½“ç³»", "è½¯ä»¶æ¶æ„ç†è®ºä½“ç³»")
                ]
            },
            iterations=10,
            warmup_iterations=2
        ))
        
        # å†…å­˜ä½¿ç”¨æ€§èƒ½æµ‹è¯•
        self.configs.append(BenchmarkConfig(
            name="å†…å­˜ä½¿ç”¨æ€§èƒ½æµ‹è¯•",
            description="æµ‹è¯•å·¥å…·é“¾çš„å†…å­˜ä½¿ç”¨æ•ˆç‡",
            test_function=self._benchmark_memory_usage,
            parameters={
                "operation_count": 1000,
                "data_size": "large"
            },
            iterations=8,
            warmup_iterations=2,
            enable_memory_monitoring=True
        ))
        
        # å¹¶å‘æ€§èƒ½æµ‹è¯•
        self.configs.append(BenchmarkConfig(
            name="å¹¶å‘æ€§èƒ½æµ‹è¯•",
            description="æµ‹è¯•å·¥å…·é“¾çš„å¹¶å‘å¤„ç†èƒ½åŠ›",
            test_function=self._benchmark_concurrent_operations,
            parameters={
                "concurrent_tasks": 10,
                "task_type": "code_generation"
            },
            iterations=5,
            warmup_iterations=1,
            enable_multithreading=True,
            thread_count=8
        ))
        
        # å¤§è§„æ¨¡æ•°æ®å¤„ç†æµ‹è¯•
        self.configs.append(BenchmarkConfig(
            name="å¤§è§„æ¨¡æ•°æ®å¤„ç†æµ‹è¯•",
            description="æµ‹è¯•å¤„ç†å¤§è§„æ¨¡ç†è®ºæ•°æ®çš„æ€§èƒ½",
            test_function=self._benchmark_large_scale_processing,
            parameters={
                "data_size": "extra_large",
                "operation_type": "analysis"
            },
            iterations=3,
            warmup_iterations=1,
            timeout=600.0
        ))
    
    def _benchmark_theory_loading(self, config: BenchmarkConfig) -> Dict[str, Any]:
        """ç†è®ºåŠ è½½æ€§èƒ½æµ‹è¯•"""
        theory_files = config.parameters.get("theory_files", [])
        file_count = config.parameters.get("file_count", 10)
        
        # æ¨¡æ‹Ÿç†è®ºæ–‡ä»¶åŠ è½½
        results = []
        for _ in range(file_count):
            start_time = time.time()
            
            # æ¨¡æ‹ŸåŠ è½½ç†è®ºæ–‡ä»¶
            theory_data = {
                "name": f"ç†è®ºæ–‡ä»¶_{_}",
                "content": "ç†è®ºå†…å®¹" * 1000,  # æ¨¡æ‹Ÿå¤§é‡å†…å®¹
                "metadata": {
                    "author": "ç³»ç»Ÿ",
                    "created": datetime.now().isoformat(),
                    "version": "1.0"
                }
            }
            
            # æ¨¡æ‹Ÿè§£æè¿‡ç¨‹
            time.sleep(0.01)  # æ¨¡æ‹Ÿè§£ææ—¶é—´
            
            end_time = time.time()
            results.append(end_time - start_time)
        
        return {
            "execution_time": sum(results),
            "file_count": file_count,
            "avg_time_per_file": sum(results) / len(results)
        }
    
    def _benchmark_code_generation(self, config: BenchmarkConfig) -> Dict[str, Any]:
        """ä»£ç ç”Ÿæˆæ€§èƒ½æµ‹è¯•"""
        languages = config.parameters.get("languages", ["python"])
        patterns = config.parameters.get("patterns", ["mvc"])
        complexity = config.parameters.get("complexity", "medium")
        
        start_time = time.time()
        
        # æ¨¡æ‹Ÿä»£ç ç”Ÿæˆ
        generated_code = {}
        for language in languages:
            for pattern in patterns:
                # æ¨¡æ‹Ÿç”Ÿæˆä»£ç 
                code = f"""
// ç”Ÿæˆçš„{language}ä»£ç  - {pattern}æ¨¡å¼
class {pattern.title()}Controller:
    def __init__(self):
        self.service = {pattern.title()}Service()
    
    def handle_request(self, request):
        return self.service.process(request)
"""
                generated_code[f"{language}_{pattern}"] = code
        
        end_time = time.time()
        
        return {
            "execution_time": end_time - start_time,
            "languages": len(languages),
            "patterns": len(patterns),
            "total_combinations": len(languages) * len(patterns)
        }
    
    def _benchmark_model_verification(self, config: BenchmarkConfig) -> Dict[str, Any]:
        """æ¨¡å‹éªŒè¯æ€§èƒ½æµ‹è¯•"""
        model_types = config.parameters.get("model_types", ["uml"])
        model_size = config.parameters.get("model_size", "medium")
        
        start_time = time.time()
        
        # æ¨¡æ‹Ÿæ¨¡å‹éªŒè¯
        verification_results = {}
        for model_type in model_types:
            # æ¨¡æ‹ŸéªŒè¯è¿‡ç¨‹
            time.sleep(0.02)  # æ¨¡æ‹ŸéªŒè¯æ—¶é—´
            
            verification_results[model_type] = {
                "valid": True,
                "issues": [],
                "performance_score": 0.95
            }
        
        end_time = time.time()
        
        return {
            "execution_time": end_time - start_time,
            "model_types": len(model_types),
            "verification_results": verification_results
        }
    
    def _benchmark_cross_theory_verification(self, config: BenchmarkConfig) -> Dict[str, Any]:
        """è·¨ç†è®ºéªŒè¯æ€§èƒ½æµ‹è¯•"""
        theory_pairs = config.parameters.get("theory_pairs", [])
        
        start_time = time.time()
        
        # æ¨¡æ‹Ÿè·¨ç†è®ºéªŒè¯
        verification_results = {}
        for theory1, theory2 in theory_pairs:
            # æ¨¡æ‹ŸéªŒè¯è¿‡ç¨‹
            time.sleep(0.05)  # æ¨¡æ‹Ÿå¤æ‚éªŒè¯æ—¶é—´
            
            verification_results[f"{theory1}_vs_{theory2}"] = {
                "consistency_score": 0.92,
                "mapping_strength": 0.88,
                "conflicts": []
            }
        
        end_time = time.time()
        
        return {
            "execution_time": end_time - start_time,
            "theory_pairs": len(theory_pairs),
            "verification_results": verification_results
        }
    
    def _benchmark_memory_usage(self, config: BenchmarkConfig) -> Dict[str, Any]:
        """å†…å­˜ä½¿ç”¨æ€§èƒ½æµ‹è¯•"""
        operation_count = config.parameters.get("operation_count", 1000)
        data_size = config.parameters.get("data_size", "large")
        
        # è·å–åˆå§‹å†…å­˜ä½¿ç”¨
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        start_time = time.time()
        
        # æ¨¡æ‹Ÿå†…å­˜å¯†é›†å‹æ“ä½œ
        data_structures = []
        for i in range(operation_count):
            # åˆ›å»ºå¤§å‹æ•°æ®ç»“æ„
            data = {
                "id": i,
                "content": "æ•°æ®å†…å®¹" * 100,
                "metadata": {"timestamp": time.time()}
            }
            data_structures.append(data)
        
        # è·å–å³°å€¼å†…å­˜ä½¿ç”¨
        peak_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        end_time = time.time()
        
        # æ¸…ç†å†…å­˜
        del data_structures
        gc.collect()
        
        final_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        return {
            "execution_time": end_time - start_time,
            "initial_memory_mb": initial_memory,
            "peak_memory_mb": peak_memory,
            "final_memory_mb": final_memory,
            "memory_increase_mb": peak_memory - initial_memory
        }
    
    def _benchmark_concurrent_operations(self, config: BenchmarkConfig) -> Dict[str, Any]:
        """å¹¶å‘æ€§èƒ½æµ‹è¯•"""
        concurrent_tasks = config.parameters.get("concurrent_tasks", 10)
        task_type = config.parameters.get("task_type", "code_generation")
        
        def task_worker(task_id: int) -> Dict[str, Any]:
            """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
            start_time = time.time()
            
            # æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œ
            if task_type == "code_generation":
                # æ¨¡æ‹Ÿä»£ç ç”Ÿæˆä»»åŠ¡
                time.sleep(0.1)
                result = f"Generated code for task {task_id}"
            elif task_type == "verification":
                # æ¨¡æ‹ŸéªŒè¯ä»»åŠ¡
                time.sleep(0.15)
                result = f"Verified model for task {task_id}"
            else:
                # é»˜è®¤ä»»åŠ¡
                time.sleep(0.05)
                result = f"Processed task {task_id}"
            
            end_time = time.time()
            return {
                "task_id": task_id,
                "execution_time": end_time - start_time,
                "result": result
            }
        
        start_time = time.time()
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘ä»»åŠ¡
        with ThreadPoolExecutor(max_workers=concurrent_tasks) as executor:
            futures = [executor.submit(task_worker, i) for i in range(concurrent_tasks)]
            results = [future.result() for future in futures]
        
        end_time = time.time()
        
        return {
            "total_execution_time": end_time - start_time,
            "concurrent_tasks": concurrent_tasks,
            "task_results": results,
            "avg_task_time": sum(r["execution_time"] for r in results) / len(results)
        }
    
    def _benchmark_large_scale_processing(self, config: BenchmarkConfig) -> Dict[str, Any]:
        """å¤§è§„æ¨¡æ•°æ®å¤„ç†æµ‹è¯•"""
        data_size = config.parameters.get("data_size", "large")
        operation_type = config.parameters.get("operation_type", "analysis")
        
        # æ ¹æ®æ•°æ®å¤§å°ç¡®å®šå¤„ç†é‡
        if data_size == "large":
            item_count = 10000
        elif data_size == "extra_large":
            item_count = 100000
        else:
            item_count = 1000
        
        start_time = time.time()
        
        # æ¨¡æ‹Ÿå¤§è§„æ¨¡æ•°æ®å¤„ç†
        processed_items = 0
        for i in range(item_count):
            # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
            data_item = {
                "id": i,
                "content": f"æ•°æ®é¡¹ {i}",
                "timestamp": time.time()
            }
            
            # æ¨¡æ‹Ÿåˆ†ææ“ä½œ
            if operation_type == "analysis":
                # æ¨¡æ‹Ÿå¤æ‚åˆ†æ
                analysis_result = {
                    "complexity": i % 10,
                    "priority": (i * 7) % 100,
                    "category": f"ç±»åˆ«_{i % 5}"
                }
            else:
                # æ¨¡æ‹Ÿç®€å•å¤„ç†
                analysis_result = {"processed": True}
            
            processed_items += 1
            
            # æ¯å¤„ç†1000ä¸ªé¡¹ç›®ï¼Œè¿›è¡Œä¸€æ¬¡å°æš‚åœ
            if i % 1000 == 0 and i > 0:
                time.sleep(0.001)
        
        end_time = time.time()
        
        return {
            "execution_time": end_time - start_time,
            "processed_items": processed_items,
            "items_per_second": processed_items / (end_time - start_time),
            "operation_type": operation_type
        }
    
    def _monitor_resources(self, duration: float) -> Dict[str, float]:
        """ç›‘æ§èµ„æºä½¿ç”¨"""
        if not self._should_monitor_resources():
            return {}
        
        cpu_samples = []
        memory_samples = []
        
        start_time = time.time()
        while time.time() - start_time < duration:
            # ç›‘æ§CPUä½¿ç”¨ç‡
            if self._should_monitor_cpu():
                cpu_percent = psutil.cpu_percent(interval=0.1)
                cpu_samples.append(cpu_percent)
            
            # ç›‘æ§å†…å­˜ä½¿ç”¨
            if self._should_monitor_memory():
                memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
                memory_samples.append(memory_mb)
            
            time.sleep(0.1)
        
        return {
            "avg_cpu_percent": statistics.mean(cpu_samples) if cpu_samples else 0,
            "peak_cpu_percent": max(cpu_samples) if cpu_samples else 0,
            "avg_memory_mb": statistics.mean(memory_samples) if memory_samples else 0,
            "peak_memory_mb": max(memory_samples) if memory_samples else 0
        }
    
    def _should_monitor_resources(self) -> bool:
        """æ˜¯å¦åº”è¯¥ç›‘æ§èµ„æº"""
        return True
    
    def _should_monitor_cpu(self) -> bool:
        """æ˜¯å¦åº”è¯¥ç›‘æ§CPU"""
        return True
    
    def _should_monitor_memory(self) -> bool:
        """æ˜¯å¦åº”è¯¥ç›‘æ§å†…å­˜"""
        return True
    
    def run_benchmark(self, config: BenchmarkConfig) -> BenchmarkResult:
        """è¿è¡Œå•ä¸ªåŸºå‡†æµ‹è¯•"""
        logger.info(f"ğŸš€ å¼€å§‹è¿è¡ŒåŸºå‡†æµ‹è¯•: {config.name}")
        
        execution_times = []
        resource_metrics = {}
        
        try:
            # é¢„çƒ­
            for i in range(config.warmup_iterations):
                logger.info(f"  é¢„çƒ­ {i+1}/{config.warmup_iterations}")
                config.test_function(config)
            
            # æ­£å¼æµ‹è¯•
            for i in range(config.iterations):
                logger.info(f"  æ‰§è¡Œæµ‹è¯• {i+1}/{config.iterations}")
                
                # å¼€å§‹èµ„æºç›‘æ§
                if config.enable_memory_monitoring or config.enable_cpu_monitoring:
                    monitor_thread = threading.Thread(
                        target=self._monitor_resources_thread,
                        args=(config.timeout, resource_metrics)
                    )
                    monitor_thread.daemon = True
                    monitor_thread.start()
                
                # æ‰§è¡Œæµ‹è¯•
                start_time = time.time()
                result = config.test_function(config)
                end_time = time.time()
                
                execution_time = end_time - start_time
                execution_times.append(execution_time)
                
                # ç­‰å¾…èµ„æºç›‘æ§å®Œæˆ
                if config.enable_memory_monitoring or config.enable_cpu_monitoring:
                    monitor_thread.join(timeout=1.0)
                
                logger.info(f"    æ‰§è¡Œæ—¶é—´: {execution_time:.4f}ç§’")
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            avg_time = statistics.mean(execution_times)
            min_time = min(execution_times)
            max_time = max(execution_times)
            std_dev = statistics.stdev(execution_times) if len(execution_times) > 1 else 0
            
            # åˆ›å»ºç»“æœ
            benchmark_result = BenchmarkResult(
                name=config.name,
                config=config,
                execution_times=execution_times,
                avg_execution_time=avg_time,
                min_execution_time=min_time,
                max_execution_time=max_time,
                std_deviation=std_dev,
                memory_usage=resource_metrics.get("avg_memory_mb"),
                peak_memory_usage=resource_metrics.get("peak_memory_mb"),
                cpu_usage=resource_metrics.get("avg_cpu_percent"),
                peak_cpu_usage=resource_metrics.get("peak_cpu_percent"),
                success=True
            )
            
            logger.info(f"âœ… åŸºå‡†æµ‹è¯•å®Œæˆ: {config.name}")
            logger.info(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time:.4f}ç§’")
            logger.info(f"  æœ€å°æ‰§è¡Œæ—¶é—´: {min_time:.4f}ç§’")
            logger.info(f"  æœ€å¤§æ‰§è¡Œæ—¶é—´: {max_time:.4f}ç§’")
            logger.info(f"  æ ‡å‡†å·®: {std_dev:.4f}ç§’")
            
            return benchmark_result
            
        except Exception as e:
            logger.error(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {config.name} - {str(e)}")
            
            return BenchmarkResult(
                name=config.name,
                config=config,
                execution_times=[],
                avg_execution_time=0,
                min_execution_time=0,
                max_execution_time=0,
                std_deviation=0,
                success=False,
                error_message=str(e)
            )
    
    def _monitor_resources_thread(self, duration: float, metrics: Dict[str, float]):
        """èµ„æºç›‘æ§çº¿ç¨‹"""
        try:
            resource_data = self._monitor_resources(duration)
            metrics.update(resource_data)
        except Exception as e:
            logger.warning(f"èµ„æºç›‘æ§å¤±è´¥: {e}")
    
    def run_all_benchmarks(self) -> List[BenchmarkResult]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        logger.info("ğŸ¯ å¼€å§‹è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•")
        
        all_results = []
        
        for config in self.configs:
            try:
                result = self.run_benchmark(config)
                all_results.append(result)
                self.results.append(result)
            except Exception as e:
                logger.error(f"åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥: {config.name} - {e}")
        
        logger.info(f"âœ… æ‰€æœ‰åŸºå‡†æµ‹è¯•å®Œæˆï¼Œå…±æ‰§è¡Œ {len(all_results)} ä¸ªæµ‹è¯•")
        return all_results
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not self.results:
            return {"error": "æ²¡æœ‰å¯ç”¨çš„åŸºå‡†æµ‹è¯•ç»“æœ"}
        
        # è®¡ç®—æ€»ä½“æ€§èƒ½æŒ‡æ ‡
        total_tests = len(self.results)
        successful_tests = len([r for r in self.results if r.success])
        failed_tests = total_tests - successful_tests
        
        # è®¡ç®—å¹³å‡æ€§èƒ½æŒ‡æ ‡
        avg_execution_times = [r.avg_execution_time for r in self.results if r.success]
        avg_memory_usage = [r.memory_usage for r in self.results if r.success and r.memory_usage]
        avg_cpu_usage = [r.cpu_usage for r in self.results if r.success and r.cpu_usage]
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "failed_tests": failed_tests,
                "success_rate": successful_tests / total_tests if total_tests > 0 else 0
            },
            "performance_metrics": {
                "avg_execution_time": statistics.mean(avg_execution_times) if avg_execution_times else 0,
                "min_execution_time": min(avg_execution_times) if avg_execution_times else 0,
                "max_execution_time": max(avg_execution_times) if avg_execution_times else 0,
                "avg_memory_usage_mb": statistics.mean(avg_memory_usage) if avg_memory_usage else 0,
                "avg_cpu_usage_percent": statistics.mean(avg_cpu_usage) if avg_cpu_usage else 0
            },
            "detailed_results": [
                {
                    "name": result.name,
                    "avg_execution_time": result.avg_execution_time,
                    "min_execution_time": result.min_execution_time,
                    "max_execution_time": result.max_execution_time,
                    "std_deviation": result.std_deviation,
                    "memory_usage_mb": result.memory_usage,
                    "cpu_usage_percent": result.cpu_usage,
                    "success": result.success,
                    "error_message": result.error_message
                }
                for result in self.results
            ],
            "timestamp": datetime.now().isoformat()
        }
        
        return report
    
    def save_results(self, filename: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"benchmark_results_{timestamp}.json"
        
        report = self.generate_performance_report()
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    def generate_visualizations(self, output_dir: str = "benchmark_visualizations"):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        if not self.results:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„åŸºå‡†æµ‹è¯•ç»“æœæ¥ç”Ÿæˆå¯è§†åŒ–")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(output_dir).mkdir(exist_ok=True)
        
        # å‡†å¤‡æ•°æ®
        successful_results = [r for r in self.results if r.success]
        
        if not successful_results:
            logger.warning("æ²¡æœ‰æˆåŠŸçš„åŸºå‡†æµ‹è¯•ç»“æœæ¥ç”Ÿæˆå¯è§†åŒ–")
            return
        
        # 1. æ‰§è¡Œæ—¶é—´å¯¹æ¯”å›¾
        plt.figure(figsize=(12, 8))
        names = [r.name for r in successful_results]
        avg_times = [r.avg_execution_time for r in successful_results]
        
        plt.bar(names, avg_times, color='skyblue', alpha=0.7)
        plt.title('åŸºå‡†æµ‹è¯•æ‰§è¡Œæ—¶é—´å¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.xlabel('æµ‹è¯•åç§°', fontsize=12)
        plt.ylabel('å¹³å‡æ‰§è¡Œæ—¶é—´ (ç§’)', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(f"{output_dir}/execution_times.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. å†…å­˜ä½¿ç”¨å¯¹æ¯”å›¾
        memory_results = [r for r in successful_results if r.memory_usage]
        if memory_results:
            plt.figure(figsize=(12, 8))
            names = [r.name for r in memory_results]
            memory_usage = [r.memory_usage for r in memory_results]
            
            plt.bar(names, memory_usage, color='lightgreen', alpha=0.7)
            plt.title('åŸºå‡†æµ‹è¯•å†…å­˜ä½¿ç”¨å¯¹æ¯”', fontsize=16, fontweight='bold')
            plt.xlabel('æµ‹è¯•åç§°', fontsize=12)
            plt.ylabel('å¹³å‡å†…å­˜ä½¿ç”¨ (MB)', fontsize=12)
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(f"{output_dir}/memory_usage.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        # 3. CPUä½¿ç”¨å¯¹æ¯”å›¾
        cpu_results = [r for r in successful_results if r.cpu_usage]
        if cpu_results:
            plt.figure(figsize=(12, 8))
            names = [r.name for r in cpu_results]
            cpu_usage = [r.cpu_usage for r in cpu_results]
            
            plt.bar(names, cpu_usage, color='lightcoral', alpha=0.7)
            plt.title('åŸºå‡†æµ‹è¯•CPUä½¿ç”¨å¯¹æ¯”', fontsize=16, fontweight='bold')
            plt.xlabel('æµ‹è¯•åç§°', fontsize=12)
            plt.ylabel('å¹³å‡CPUä½¿ç”¨ç‡ (%)', fontsize=12)
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(f"{output_dir}/cpu_usage.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        # 4. æ€§èƒ½é›·è¾¾å›¾
        if len(successful_results) >= 3:
            plt.figure(figsize=(10, 10))
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            categories = ['æ‰§è¡Œé€Ÿåº¦', 'å†…å­˜æ•ˆç‡', 'CPUæ•ˆç‡', 'ç¨³å®šæ€§', 'å¯æ‰©å±•æ€§']
            
            # æ ‡å‡†åŒ–æŒ‡æ ‡ (0-1èŒƒå›´)
            max_time = max(r.avg_execution_time for r in successful_results)
            max_memory = max(r.memory_usage or 0 for r in successful_results)
            max_cpu = max(r.cpu_usage or 0 for r in successful_results)
            
            # é€‰æ‹©å‰3ä¸ªæµ‹è¯•è¿›è¡Œå¯¹æ¯”
            top_3_results = successful_results[:3]
            
            for i, result in enumerate(top_3_results):
                # è®¡ç®—å„é¡¹æŒ‡æ ‡ (è¶Šå°è¶Šå¥½ï¼Œæ‰€ä»¥ç”¨1å‡å»æ ‡å‡†åŒ–å€¼)
                speed_score = 1 - (result.avg_execution_time / max_time)
                memory_score = 1 - ((result.memory_usage or 0) / max_memory)
                cpu_score = 1 - ((result.cpu_usage or 0) / max_cpu)
                stability_score = 1 - (result.std_deviation / result.avg_execution_time) if result.avg_execution_time > 0 else 0
                scalability_score = 0.8  # å‡è®¾å€¼
                
                values = [speed_score, memory_score, cpu_score, stability_score, scalability_score]
                
                # ç»˜åˆ¶é›·è¾¾å›¾
                angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
                values += values[:1]  # é—­åˆå›¾å½¢
                angles += angles[:1]
                
                plt.polar(angles, values, 'o-', linewidth=2, label=result.name, alpha=0.7)
                plt.fill(angles, values, alpha=0.1)
            
            plt.xticks(angles[:-1], categories)
            plt.ylim(0, 1)
            plt.title('æ€§èƒ½é›·è¾¾å›¾å¯¹æ¯”', fontsize=16, fontweight='bold', pad=20)
            plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
            plt.tight_layout()
            plt.savefig(f"{output_dir}/performance_radar.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        logger.info(f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆåˆ°: {output_dir}")
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        if not self.results:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„åŸºå‡†æµ‹è¯•ç»“æœ")
            return
        
        print("\n" + "="*80)
        print("ğŸ¯ FORMALUNIFIED æ€§èƒ½åŸºå‡†æµ‹è¯•æ‘˜è¦")
        print("="*80)
        
        # æ€»ä½“ç»Ÿè®¡
        total_tests = len(self.results)
        successful_tests = len([r for r in self.results if r.success])
        failed_tests = total_tests - successful_tests
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"  æˆåŠŸæµ‹è¯•: {successful_tests}")
        print(f"  å¤±è´¥æµ‹è¯•: {failed_tests}")
        print(f"  æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%")
        
        if successful_tests > 0:
            # æ€§èƒ½ç»Ÿè®¡
            avg_times = [r.avg_execution_time for r in self.results if r.success]
            avg_memory = [r.memory_usage for r in self.results if r.success and r.memory_usage]
            avg_cpu = [r.cpu_usage for r in self.results if r.success and r.cpu_usage]
            
            print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
            print(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {statistics.mean(avg_times):.4f}ç§’")
            print(f"  æœ€å¿«æ‰§è¡Œæ—¶é—´: {min(avg_times):.4f}ç§’")
            print(f"  æœ€æ…¢æ‰§è¡Œæ—¶é—´: {max(avg_times):.4f}ç§’")
            
            if avg_memory:
                print(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {statistics.mean(avg_memory):.2f}MB")
            if avg_cpu:
                print(f"  å¹³å‡CPUä½¿ç”¨: {statistics.mean(avg_cpu):.1f}%")
        
        # è¯¦ç»†ç»“æœ
        print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for result in self.results:
            status = "âœ…" if result.success else "âŒ"
            print(f"  {status} {result.name}")
            if result.success:
                print(f"    æ‰§è¡Œæ—¶é—´: {result.avg_execution_time:.4f}ç§’ (Â±{result.std_deviation:.4f})")
                if result.memory_usage:
                    print(f"    å†…å­˜ä½¿ç”¨: {result.memory_usage:.2f}MB")
                if result.cpu_usage:
                    print(f"    CPUä½¿ç”¨: {result.cpu_usage:.1f}%")
            else:
                print(f"    é”™è¯¯: {result.error_message}")
        
        print("\n" + "="*80)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨FormalUnifiedæ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶")
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å¥—ä»¶
    benchmark_suite = PerformanceBenchmarkSuite()
    
    # è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
    results = benchmark_suite.run_all_benchmarks()
    
    # ç”ŸæˆæŠ¥å‘Š
    benchmark_suite.print_summary()
    
    # ä¿å­˜ç»“æœ
    benchmark_suite.save_results()
    
    # ç”Ÿæˆå¯è§†åŒ–
    benchmark_suite.generate_visualizations()
    
    print("\nğŸ‰ æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main() 